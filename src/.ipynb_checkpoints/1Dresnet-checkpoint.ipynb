{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from create_datasets import *\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_edges = np.load('../datasets/final_edges.dump', allow_pickle=True)\n",
    "data = generate_fingerprints(final_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv1dPadSame(nn.Module):\n",
    "    \"\"\"\n",
    "    extend nn.Conv1d to support SAME padding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1):\n",
    "        super(MyConv1dPadSame, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels=self.in_channels, \n",
    "            out_channels=self.out_channels, \n",
    "            kernel_size=self.kernel_size, \n",
    "            stride=self.stride, \n",
    "            groups=self.groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        net = x\n",
    "        \n",
    "        # compute pad shape\n",
    "        in_dim = net.shape[-1]\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n",
    "        pad_left = p // 2\n",
    "        pad_right = p - pad_left\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n",
    "        \n",
    "        net = self.conv(net)\n",
    "\n",
    "        return net\n",
    "        \n",
    "class MyMaxPool1dPadSame(nn.Module):\n",
    "    \"\"\"\n",
    "    extend nn.MaxPool1d to support SAME padding\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(MyMaxPool1dPadSame, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = 1\n",
    "        self.max_pool = torch.nn.MaxPool1d(kernel_size=self.kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        net = x\n",
    "        \n",
    "        # compute pad shape\n",
    "        in_dim = net.shape[-1]\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n",
    "        pad_left = p // 2\n",
    "        pad_right = p - pad_left\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n",
    "        \n",
    "        net = self.max_pool(net)\n",
    "        \n",
    "        return net\n",
    "    \n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Basic Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, downsample, use_bn, use_do, is_first_block=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.downsample = downsample\n",
    "        if self.downsample:\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            self.stride = 1\n",
    "        self.is_first_block = is_first_block\n",
    "        self.use_bn = use_bn\n",
    "        self.use_do = use_do\n",
    "\n",
    "        # the first conv\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.do1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = MyConv1dPadSame(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=self.stride,\n",
    "            groups=self.groups)\n",
    "\n",
    "        # the second conv\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.do2 = nn.Dropout(p=0.5)\n",
    "        self.conv2 = MyConv1dPadSame(\n",
    "            in_channels=out_channels, \n",
    "            out_channels=out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=1,\n",
    "            groups=self.groups)\n",
    "                \n",
    "        self.max_pool = MyMaxPool1dPadSame(kernel_size=self.stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        # the first conv\n",
    "        out = x\n",
    "        if not self.is_first_block:\n",
    "            if self.use_bn:\n",
    "                out = self.bn1(out)\n",
    "            out = self.relu1(out)\n",
    "            if self.use_do:\n",
    "                out = self.do1(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        # the second conv\n",
    "        if self.use_bn:\n",
    "            out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        if self.use_do:\n",
    "            out = self.do2(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # if downsample, also downsample identity\n",
    "        if self.downsample:\n",
    "            identity = self.max_pool(identity)\n",
    "            \n",
    "        # if expand channel, also pad zeros to identity\n",
    "        if self.out_channels != self.in_channels:\n",
    "            identity = identity.transpose(-1,-2)\n",
    "            ch1 = (self.out_channels-self.in_channels)//2\n",
    "            ch2 = self.out_channels-self.in_channels-ch1\n",
    "            identity = F.pad(identity, (ch1, ch2), \"constant\", 0)\n",
    "            identity = identity.transpose(-1,-2)\n",
    "        \n",
    "        # shortcut\n",
    "        out += identity\n",
    "\n",
    "        return out\n",
    "    \n",
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "        X: (n_samples, n_channel, n_length)\n",
    "        Y: (n_samples)\n",
    "        \n",
    "    Output:\n",
    "        out: (n_samples)\n",
    "        \n",
    "    Pararmetes:\n",
    "        in_channels: dim of input, the same as n_channel\n",
    "        base_filters: number of filters in the first several Conv layer, it will double at every 4 layers\n",
    "        kernel_size: width of kernel\n",
    "        stride: stride of kernel moving\n",
    "        groups: set larget to 1 as ResNeXt\n",
    "        n_block: number of blocks\n",
    "        n_classes: number of classes\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=1, base_filters=64, kernel_size=3, stride=1, groups=1, n_block=5, n_classes=2, downsample_gap=2, increasefilter_gap=4, use_bn=True, use_do=True, verbose=False):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.n_block = n_block\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.use_bn = use_bn\n",
    "        self.use_do = use_do\n",
    "\n",
    "        self.downsample_gap = downsample_gap # 2 for base model\n",
    "        self.increasefilter_gap = increasefilter_gap # 4 for base model\n",
    "\n",
    "        # first block\n",
    "        self.first_block_conv = MyConv1dPadSame(in_channels=in_channels, out_channels=base_filters, kernel_size=self.kernel_size, stride=1)\n",
    "        self.first_block_bn = nn.BatchNorm1d(base_filters)\n",
    "        self.first_block_relu = nn.ReLU()\n",
    "        out_channels = base_filters\n",
    "                \n",
    "        # residual blocks\n",
    "        self.basicblock_list = nn.ModuleList()\n",
    "        for i_block in range(self.n_block):\n",
    "            # is_first_block\n",
    "            if i_block == 0:\n",
    "                is_first_block = True\n",
    "            else:\n",
    "                is_first_block = False\n",
    "            # downsample at every self.downsample_gap blocks\n",
    "            if i_block % self.downsample_gap == 1:\n",
    "                downsample = True\n",
    "            else:\n",
    "                downsample = False\n",
    "            # in_channels and out_channels\n",
    "            if is_first_block:\n",
    "                in_channels = base_filters\n",
    "                out_channels = in_channels\n",
    "            else:\n",
    "                # increase filters at every self.increasefilter_gap blocks\n",
    "                in_channels = int(base_filters*2**((i_block-1)//self.increasefilter_gap))\n",
    "                if (i_block % self.increasefilter_gap == 0) and (i_block != 0):\n",
    "                    out_channels = in_channels * 2\n",
    "                else:\n",
    "                    out_channels = in_channels\n",
    "            \n",
    "            tmp_block = BasicBlock(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=self.kernel_size, \n",
    "                stride = self.stride, \n",
    "                groups = self.groups, \n",
    "                downsample=downsample, \n",
    "                use_bn = self.use_bn, \n",
    "                use_do = self.use_do, \n",
    "                is_first_block=is_first_block)\n",
    "            self.basicblock_list.append(tmp_block)\n",
    "\n",
    "        # final prediction\n",
    "        self.final_bn = nn.BatchNorm1d(out_channels)\n",
    "        self.final_relu = nn.ReLU(inplace=True)\n",
    "        # self.do = nn.Dropout(p=0.5)\n",
    "        self.dense = nn.Linear(out_channels, n_classes)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        \n",
    "        out = self.first_block_conv(x)\n",
    " \n",
    "        if self.use_bn:\n",
    "            out = self.first_block_bn(out)\n",
    "        out = self.first_block_relu(out)\n",
    "        \n",
    "        # residual blocks, every block has two conv\n",
    "        for i_block in range(self.n_block):\n",
    "            net = self.basicblock_list[i_block]           \n",
    "            out = net(out)\n",
    "\n",
    "        # final prediction\n",
    "        if self.use_bn:\n",
    "            out = self.final_bn(out)\n",
    "        out = self.final_relu(out)\n",
    "        out = out.mean(-1)\n",
    "\n",
    "        # out = self.do(out)\n",
    "        out = self.dense(out)\n",
    "    \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNet(nn.Module):\n",
    "    def __init__(self, finger_print_model, graph_embedding_model, combined_model):\n",
    "        super().__init__()\n",
    "        self.FP_model = finger_print_model\n",
    "        self.GE_model = graph_embedding_model\n",
    "        self.CB_model = combined_model\n",
    "    \n",
    "    def forward(self, fp, ge):\n",
    "        fp_out = self.FP_model(fp)\n",
    "        ge_out = self.GE_model(ge)\n",
    "        inp = torch.cat((fp_out, ge_out), 1)\n",
    "        inp = inp.unsqueeze(1)\n",
    "        out = self.CB_model(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ce473",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# device= 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LinkDataset(data)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train , test = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "BATCH_SIZE = 512\n",
    "trainloader = DataLoader(train, num_workers = 16, batch_size= BATCH_SIZE)\n",
    "testloader = DataLoader(test, num_workers = 16, batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullNet(ResNet1D(n_classes=512), \n",
    "                ResNet1D(n_classes=512), \n",
    "                ResNet1D(n_classes=2))\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df09a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for fp, ge, label in testloader:\n",
    "        output = model(fp.float().to(device),ge.float().to(device))\n",
    "        loss = criterion(output, label.float().to(device))\n",
    "        test_loss+=loss.item()\n",
    "        for i in range(len(output)):\n",
    "            pred = output[i].argmax().item()\n",
    "            true = label[i].argmax().item()\n",
    "            preds.append(pred)\n",
    "            trues.append(true)\n",
    "    model.train()\n",
    "#     print(\"Accuracy\", accuracy_score(preds, trues))\n",
    "    return accuracy_score(preds, trues), test_loss / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "num_epochs= 50\n",
    "best_acc = 0.0\n",
    "acc_list = []\n",
    "for epoch in tqdm(range(1, num_epochs)):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for fp, ge, label in trainloader:\n",
    "        batch_id +=1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(fp.float().to(device),ge.float().to(device))\n",
    "        loss = criterion(output, label.float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() \n",
    "    \n",
    "        print(f'Epoch:{epoch} batch {batch_id}/{len(trainloader)} loss:{loss.item()}', end='\\r')\n",
    "    \n",
    "    acc, test_loss = eval(model, testloader)\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        print(\"Improved Accuracy is\", acc )\n",
    "        torch.save(model, 'SAVED_MODELS/Resnet-bestmodel_1.pt')\n",
    "        with open('SAVED_MODELS/Resnet-bestmodel_1.txt', 'w') as f:\n",
    "            print(model.eval() , \"Accuracy\" , acc, file=f)\n",
    "\n",
    "    else:\n",
    "        model = torch.load('SAVED_MODELS/Resnet-bestmodel_1.pt')\n",
    "    \n",
    "    print()\n",
    "    print(\"Train loss: \",train_loss/len(trainloader))\n",
    "    print(\"Test  loss: \",test_loss)\n",
    "    \n",
    "    train_losses.append(train_loss/len(trainloader))\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfe225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
